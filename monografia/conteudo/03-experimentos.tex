%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Experimentos}
\label{chap:experimentos}

Para cada um dos modelos selecionados foram realizados diferentes experimentos
alterando parâmetros com o objetivo de obter um melhor 
desempenho. Neste seção estão descritos os experimentos realizados de acordo 
com o modelo.


\section{Regressao linear}

A regressão linear é utilizada 
neste estudo como base para comparar o desempenho dos
outros modelos por ser um modelo mais simples de \textit{machine learning}. 
Foram testados métodos de normalização  e transformação de 
dados, além da 
remoção de variáveis de alta correlação conforme 
descrito na tabela \ref{tab:exp-reg-lin}. 

\begin{table}
    \centering
    \caption{Experimentos realizados na regressão linear}
    \begin{tabular}{llll}
        \toprule
        Experimento & Transformação nos dados     & Período de dados & Remoção de variáveis  \\
        \midrule
        A           & nenhum & de 2003 até 2019            & não~                                     \\
        B           & \textit{Standard scaler}~            & de 2003 até 2019            & não~ ~                                   \\
        C           & \textit{MinMax scaler}~ ~            & de 2003 até 2019            & não~ ~ ~                                 \\
        D           & \textit{Power Transformer}           & de 2003 até 2019            & não                                      \\
        E           & nenhum & de 2003 até 2019            & sim\footnote{As variáveis removidas foram: PIB \textit{per capita}, INCC, IGP, taxa Selic, IDH Educação e IDH Longevidade, 
        NFSP, preço do saco de cimento e preço da tonelada de cimento}                                      \\
        \bottomrule
    \end{tabular}
    \label{tab:exp-reg-lin}
\end{table}

O desempenho do modelo em cada um dos experimentos 
encontra-se na 
tabela \ref{tab:res_reg_lin}:

\begin{table}[H]
    \centering
    \begin{tabular}{llll}
        \toprule
        Experimento & MAE     & RMSE    & MAPE \\
        \midrule
        A           & 35148.6 & 55391.1 & 0.6  \\
        B           & 35148.6 & 55391.1 & 0.6  \\
        C           & 35148.6 & 55391.1 & 0.6  \\
        D           & 49073.6 & 67446.7 & 1.05 \\
        E           & \textbf{26435.6} & \textbf{38970.6} & \textbf{0.32} \\
        \bottomrule
    \end{tabular}
    \caption{Desempenho dos modelos de regressão linear}
    \label{tab:res_reg_lin}
\end{table}

Observa-se que não houve alteração na performance ao normalizar 
os dados com \textit{standard scaler} ou \textit{minmax scaler},
ao utilizar \textit{power transformer}, contudo, houve piora 
no desempenho. Ressalta-se a grande melhoria nas métricas ao 
remover variáveis de alta correlação tanto nas métricas de erro 
quantitativo, como o MAE e RMSE, quanto proporcionalmente ao 
atingir 32\% de erro percentual médio.

Foi realizado o teste de remover variáveis de alta correlação, 
pois segundo \cite{corr_reg_lin}, esse tipo de \textit{feature}
pode prejudicar no modelo por aumentar a variabilidade
dos coeficientes em relação à amostra, dessa forma, os coeficientes 
calculados pelo modelo podem ter um alto grau de variação de uma 
amostra para outra.

Na seção \ref{chap:resultados}, estão apresentados os 
resultados e previsões do modelo de regressão linear com
melhor desempenho dentre os testados no modelo, o
experimento E, sem   
normalização de dados e removendo atributos com alta 
correlação.

\section{Redes \textit{feed forward}}

As redes neurais \textit{feed forward} podem ser construídas
com várias arquiteturas e configurações. Neste estudo, testou-se
alterar a quantidade de camadas da rede, o número neurônios em 
cada camada, a função de ativação utilizada, além da quantidade
de \textit{epoch}\footnote{Uma \textit{epoch} é uma passada 
pelos dados de entrada (\cite{dl-oreilly})} utilizada no treinamento.

Os valores testados para cada um dos parâmetros são dados na tabela \ref{tab:param-mlp}:

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        Parâmetro           & Valores   \\
        \midrule
        Número de camadas   & 1,2,3 e 4 \\
        Número de neurônios & 32,64,128 e 256       \\
        Função de ativação  & ReLU e \textit{swish}  \\
        Epochs              & 50, 100 e 150      \\
        Pré-processamento de dados & normalização, \textit{min-max scaler} e \textit{power transformer}\\
        \bottomrule
    \end{tabular}
    \caption{Parâmetros testados nas redes \textit{feed forward}}
    \label{tab:param-mlp}
\end{table} 

Cada experimento corresponde a uma combinação dos parâmetros 
da tabela \ref{tab:param-mlp}, por exemplo, um modelo de uma 
camada de 32 neurônios, função de ativação ReLU e 50 \textit{epochs}.
Os resultados dos experimentos foram separados de acordo 
com o número de camadas na rede para melhor visualização. O desempenho dos 
melhores modelos com melhor em relação a outros com o mesmo número de 
camadas é dado na tabela \ref{tab:res-mlp}.

\begin{table}[H]
    \centering
    \caption{Experimentos com melhor desempenho segundo o número de camadas das redes \textit{feed forward}}
    \begin{tabular}{llll}
    \toprule
    Número de camadas  & MAE & RMSE & MAPE \\
    \midrule
    1 & 51317  & 87398  & 0.50\\
    2 & 33028  & 60233 & 0.34 \\
    \textbf{3} & \textbf{26880}  & \textbf{50830} & \textbf{0.27} \\
    4 & 25977  & 42925 & 0.31 \\
    \bottomrule
    \end{tabular}
    \label{tab:res-mlp}
\end{table}

Interessante notar que ao priorizar os erros absolutos (MAE e RMSE), o modelo 
com quatro camadas apresentou um desempenho ligeiramente
melhor que o de três camadas, contudo, possui maior erro percentual.
Assim, por mais que os erros absolutos sejam menores, ao levar em 
consideração o valor que o modelo pretende prever, os erros desse 
modelo são maiores que os gerados pelo modelo de três camadas. 

Neste trabalho, priorizou-se modelos com menor erro percentual (MAPE), então 
o modelo com melhor desempenho entre as redes \textit{multilayer feed forward}
foi o de três camadas. Esse modelo utiliza a função de ativação \textit{swish},
o método de normalizaçõa para pré-processar os dados e foi treinado com 100 
\textit{epochs}, além disso a primeira camada oculta possui 256 neurônios, a 
segunda, 64 e a terceira, 32.

\section{Redes recorrentes}

Neste trabalho, foram utilizadas redes recorrentes LSTM e GRU, para ambas as 
redes, realizaram-se experimentos alterando os mesmos parâmetros, a saber,
número de camadas, número de neurônios, função de ativação, número de \textit{epochs}
e método de pré-processamento de dados. Na tabela \ref{tab:param-rnn} encontra-se 
a relação dos valores testados para cada parâmetro.

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        Parâmetro           & Valores   \\
        \midrule
        Número de camadas   & 1 e 2 \\
        Número de neurônios & 32,64,128 e 256       \\
        Função de ativação  & tangente hiperbólica\footnote{Por padrão, as redes LSTM e GRU utilizam a tangente hiperbólica ($tanh$) como função de ativação.}, ReLU e \textit{swish}  \\
        \textit{Epochs}              & 50, 100 e 150      \\
        Pré-processamento de dados & normalização, \textit{min-max scaler} e \textit{power transformer}\\
        \textit{time step}\footnote{O \textit{time step} é o tamanho da sequência que a rede recebe como entrada.}  & 3 e 5 \\
        \bottomrule
    \end{tabular}
    \caption{Parâmetros testados nas redes recorrentes}
    \label{tab:param-rnn}
\end{table} 

Foram realizados os mesmos experimentos para ambas as redes, assim como para 
as redes neurais tradicionais, os resultados foram agrupados de acordo com 
o número de camadas da rede. O desempenho dos modelos com melhores resultados 
estão na tabela \ref{tab:res-rnn}.

\begin{table}[H]
    \centering
    \caption{Experimentos com melhor desempenho segundo o número de camadas das redes recorrentes}
    \begin{tabular}{lllll}
    \toprule
    Número de camadas & Rede & MAE & RMSE & MAPE \\
    \midrule
    \textbf{1} & \textbf{LSTM} & \textbf{22694}  & \textbf{43918} & \textbf{0.19}\\
    1 & GRU  & 26751  & 52281  & 0.24 \\
    2 & LSTM & 25002  & 47565 & 0.19  \\
    2 & GRU  & 24992  & 50602  & 0.20 \\
    \bottomrule
    \end{tabular}
    \label{tab:res-rnn}
\end{table}

Observa-se que as redes LSTM obtiveram um melhor desempenho, em geral, que as
redes GRU e que a rede LSTM mais simples, com uma camada, fez as previsões 
mais precisas. A rede recorrente com melhor desempenho, então, é uma rede LSTM 
de uma camada com 32 neurônios, que utiliza a ReLU como função de ativação, com 
150 \textit{epochs} e normalização como método de processamento de dados.

\section{Redes bidirecionais}

As redes bidirecionais utilizam duas redes recorrentes para 
realizar as previsões. Neste estudo, realizaram-se experimentos com redes
LSTM e GRU, alterou-se também o número de camadas, a quantidade de neurônios
nas camadas, a função de ativação utilizada nas redes, o método de pré-processamento
de dados e a quantidade de \textit{epochs}. A relação dos valores testados 
para cada parâmetro está na tabela \ref{tab:param-bi}.

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        Parâmetro           & Valores   \\
        \midrule
        Número de camadas   & 1 e 2 \\
        Número de neurônios & 32, 64, 128 e 256       \\
        Função de ativação  & tangente hiperbólica\footnote{Por padrão, as redes LSTM e GRU utilizam a tangente hiperbólica ($tanh$) como função de ativação.}, ReLU e \textit{swish}  \\
        \textit{Epochs}              & 50, 100 e 150      \\
        Pré-processamento de dados & normalização, \textit{min-max scaler} e \textit{power transformer}\\
        \textit{time step}\footnote{O \textit{time step} é o tamanho da sequência que a rede recebe como entrada.}  & 3 e 5 \\
        \bottomrule
    \end{tabular}
    \caption{Parâmetros testados nas redes bidirecionais}
    \label{tab:param-bi}
\end{table} 

O resultado dos modelos foi agrupado de acordo com o número de camadas da 
rede e com a categoria de rede recorrente escolhida (LSTM ou GRU). A relação dos modelos com 
melhor desempenho é dada na tabela \ref{tab:res-bi}.

\begin{table}[H]
    \centering
    \caption{Experimentos com melhor desempenho das redes bidirecionais}
    \begin{tabular}{lllll}
    \toprule
    Número de camadas & Rede & MAE & RMSE & MAPE \\
    \midrule
    \textbf{1} & \textbf{LSTM} & \textbf{19185}  & \textbf{33942} & \textbf{0.17}\\
    1 & GRU  & 19952  & 33979  & 0.18 \\
    2 & LSTM & 22964  & 38688 & 0.20  \\
    2 & GRU  & 20993  & 35817  & 0.20 \\
    \bottomrule
    \end{tabular}
    \label{tab:res-bi}
\end{table}

Observa-se que, o modelo com melhor desempenho das redes bidirecionais 
utiliza uma camada de rede LSTM de 2 neurônios para fazer as previsões, além disso, o experimento foi 
realizado com 150 \textit{epochs} e com o método de pré-processamento de dados 
\textit{min-max scaler}.